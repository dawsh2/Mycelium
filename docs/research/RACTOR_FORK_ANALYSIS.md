# Ractor Fork Analysis: Adding Arc<T> Fast Path

## Executive Summary

**Question**: Should we fork Ractor and add Arc<T> optimization for local actors?

**Answer**: **Maybe, but it's not simple.** The changes required are more invasive than they initially appear.

---

## What We'd Need to Change

### 1. **Message Boxing Layer** (Medium Complexity)

**Current**: `BoxedMessage` has two variants:
```rust
pub struct BoxedMessage {
    msg: Option<Box<dyn Any + Send>>,           // Local (same process)
    serialized_msg: Option<SerializedMessage>,  // Remote (any node)
}
```

**Required**: Add third variant for Arc<T>:
```rust
pub struct BoxedMessage {
    msg: Option<Box<dyn Any + Send>>,           // Local (same process)
    arc_msg: Option<Arc<dyn Any + Send + Sync>>, // NEW: Same machine, zero-copy
    serialized_msg: Option<SerializedMessage>,  // Remote (network)
}
```

**Impact**: Every message handling path must check 3 variants instead of 2.

---

### 2. **Actor Location Awareness** (High Complexity)

**Current**: `ActorId` is either:
```rust
pub enum ActorId {
    Local { pid: u64 },                    // Same process
    Remote { node_id: NodeId, pid: u64 },  // Different node
}
```

**Required**: Add "same machine" distinction:
```rust
pub enum ActorId {
    Local { pid: u64 },                           // Same process (Arc fast path)
    LocalMachine { node_id: NodeId, pid: u64 },  // Same machine (Unix socket)
    Remote { node_id: NodeId, pid: u64 },        // Different machine (TCP)
}
```

**Problem**: How do we know if `node_id` is on the same machine?

**Solutions**:
1. **Static config** - Manually define which nodes are local
2. **Discovery protocol** - Nodes announce their hostname/IP
3. **Connection type** - Infer from transport (Unix socket = local)

**Impact**: `ractor_cluster` needs significant changes to track node locality.

---

### 3. **Transport Layer Abstraction** (High Complexity)

**Current**: `ractor_cluster` assumes TCP:
```rust
// In ractor_cluster/src/node/session.rs
async fn send_message(&self, msg: NodeMessage) {
    // Serialize to protobuf
    let bytes = msg.encode_to_vec();
    // Send over TCP
    self.tcp_stream.write_all(&bytes).await?;
}
```

**Required**: Pluggable transport:
```rust
pub trait Transport: Send + Sync {
    async fn send(&self, msg: TransportMessage) -> Result<(), TransportErr>;
    async fn recv(&self) -> Result<TransportMessage, TransportErr>;
}

pub enum TransportMessage {
    Arc(Arc<dyn Any + Send + Sync>),  // Zero-copy local
    Bytes(Vec<u8>),                    // Serialized remote
}

pub struct TcpTransport { ... }
pub struct UnixSocketTransport { ... }
pub struct ArcTransport { ... }  // NEW: In-memory channel
```

**Impact**: `NodeSession` becomes transport-agnostic. Major refactor.

---

### 4. **Message Trait Changes** (Medium Complexity)

**Current**: Message trait has:
```rust
pub trait Message: Any + Send + Sized + 'static {
    fn serializable() -> bool { false }
    fn serialize(self) -> Result<SerializedMessage, BoxedDowncastErr>;
    fn deserialize(bytes: SerializedMessage) -> Result<Self, BoxedDowncastErr>;
}
```

**Required**: Add Arc support:
```rust
pub trait Message: Any + Send + Sync + Sized + 'static {  // + Sync required
    fn serializable() -> bool { false }
    fn arc_shareable() -> bool { true }  // NEW: Can use Arc?
    
    fn serialize(self) -> Result<SerializedMessage, BoxedDowncastErr>;
    fn deserialize(bytes: SerializedMessage) -> Result<Self, BoxedDowncastErr>;
    
    fn into_arc(self) -> Arc<dyn Any + Send + Sync>;  // NEW
}
```

**Breaking Change**: All messages must now be `Sync`, not just `Send`.

**Impact**: Existing Ractor code might break if messages aren't `Sync`.

---

### 5. **Actor Spawning API** (Low Complexity)

**Current**:
```rust
Actor::spawn(name, actor, args).await?;
```

**Required**: Specify transport preference:
```rust
let config = ActorConfig::default()
    .with_transport_preference(TransportPreference::Adaptive);  // NEW

Actor::spawn_with_config(name, actor, args, config).await?;
```

**Impact**: Backwards compatible if we make `Adaptive` the default.

---

## Architectural Challenges

### Challenge 1: Node Locality Discovery

**Problem**: How does a node know another node is on the same machine?

**Option A: Static Configuration**
```toml
# config/topology.toml
[nodes.node1]
location = "local"  # Same machine

[nodes.node2]
location = "local"

[nodes.node3]
location = "remote"  # Different machine
```

**Pros**: Simple, explicit
**Cons**: Manual, error-prone, doesn't handle dynamic deployments

---

**Option B: Discovery Protocol**
```rust
// Nodes announce their hostname during handshake
struct NodeHandshake {
    node_id: NodeId,
    hostname: String,      // NEW
    listen_addr: SocketAddr,
}

// Compare with local hostname
impl NodeSession {
    fn is_local_machine(&self) -> bool {
        self.remote_hostname == local_hostname()
    }
}
```

**Pros**: Automatic, works in dynamic environments
**Cons**: Requires protocol changes, hostname comparison is fuzzy

---

**Option C: Connection Type Detection**
```rust
impl NodeSession {
    fn is_local_machine(&self) -> bool {
        matches!(self.transport, Transport::UnixSocket(_))
    }
}
```

**Pros**: Clean, direct
**Cons**: Couples locality to transport choice (what if you want TCP on same machine?)

---

### Challenge 2: Arc<T> Type Erasure

**Problem**: `Arc<dyn Any>` loses type information. How do we downcast safely?

**Current Ractor approach** (for `Box<dyn Any>`):
```rust
fn from_boxed(mut m: BoxedMessage) -> Result<Self, BoxedDowncastErr> {
    match m.msg.take() {
        Some(m) => {
            if m.is::<Self>() {
                Ok(*m.downcast::<Self>().unwrap())
            } else {
                Err(BoxedDowncastErr)
            }
        }
        _ => Err(BoxedDowncastErr),
    }
}
```

**Arc variant**:
```rust
fn from_boxed_arc(m: Arc<dyn Any + Send + Sync>) -> Result<Arc<Self>, BoxedDowncastErr> {
    m.downcast::<Self>()  // ‚ùå Doesn't work! Arc::downcast doesn't exist
}
```

**The issue**: `Arc::downcast()` isn't in std. We need:
1. Use `arc-swap` crate's `ArcSwapAny`
2. Or wrap in our own newtype: `struct ArcMessage(Arc<dyn Any>)` with custom downcast
3. Or use trait objects differently

**Impact**: Non-trivial implementation complexity.

---

### Challenge 3: Sync Requirement Breaking Change

**Current**: Ractor messages only need `Send`:
```rust
pub trait Message: Any + Send + Sized + 'static { ... }
```

**With Arc**: Need `Send + Sync`:
```rust
pub trait Message: Any + Send + Sync + Sized + 'static { ... }
```

**Why?** `Arc<T>` requires `T: Send + Sync` to be shareable across threads.

**Breaking change**: Existing Ractor code with `!Sync` messages would break.

**Example problematic message**:
```rust
struct MyMessage {
    data: Rc<String>,  // ‚ùå Not Sync
}
```

**Migration path**: Force all users to make messages `Sync`, which might be hard.

---

## Comparison: Fork Ractor vs Build Custom

### Fork Ractor

**Pros**:
- ‚úÖ Get excellent supervision trees for free
- ‚úÖ Get registry, spawning, lifecycle for free
- ‚úÖ Production-tested foundation
- ‚úÖ Meta's battle-testing

**Cons**:
- ‚ùå **Major invasive changes** (5 subsystems)
- ‚ùå **Breaking changes** to Message trait (+ Sync)
- ‚ùå **Upstream divergence** - our fork drifts from mainline
- ‚ùå **Maintenance burden** - keep syncing with upstream
- ‚ùå **Architectural complexity** - working around design decisions
- ‚ùå **Arc downcasting** complexity

**Estimated effort**: 6-8 weeks

---

### Build Custom on Tokio

**Pros**:
- ‚úÖ **Full control** over architecture
- ‚úÖ **Designed for Arc<T>** from day 1
- ‚úÖ **No breaking changes** to maintain
- ‚úÖ **No upstream sync** needed
- ‚úÖ **Simpler** - only build what we need
- ‚úÖ **Arc<T> is natural**, not retrofitted

**Cons**:
- ‚ùå Build supervision ourselves (~500 LOC)
- ‚ùå Build registry ourselves (~200 LOC)
- ‚ùå Less battle-tested initially

**Estimated effort**: 4-5 weeks for MVP, 8-10 weeks for full

---

## Decision Matrix

| Criteria | Fork Ractor | Build Custom | Winner |
|----------|-------------|--------------|--------|
| **Time to MVP** | 6-8 weeks | 4 weeks | Custom |
| **Time to Production** | 8-10 weeks | 10-12 weeks | Tie |
| **Arc<T> Performance** | ~300ns (retrofit) | ~200ns (native) | Custom |
| **Supervision Quality** | ‚úÖ‚úÖ Excellent | ‚úÖ Good (DIY) | Ractor |
| **Maintenance Burden** | ‚ùå High (upstream sync) | ‚úÖ Low (ours) | Custom |
| **Architectural Fit** | ‚ö†Ô∏è Forced fit | ‚úÖ Perfect fit | Custom |
| **Breaking Changes Risk** | ‚ùå High (Sync req) | ‚úÖ None | Custom |
| **Code Complexity** | ‚ùå High (5 systems) | ‚úÖ Moderate | Custom |
| **Battle-Testing** | ‚úÖ‚úÖ Meta production | ‚ö†Ô∏è Ours | Ractor |
| **Learning Curve** | ‚ö†Ô∏è Understand Ractor internals | ‚úÖ Our design | Custom |

**Score**: Custom wins 6-3

---

## The Hybrid Approach

**Best of both worlds?**

### Option: Use Ractor for Control Plane, Custom for Data Plane

```rust
// Control plane (strategy coordination, risk, config)
// Uses Ractor for supervision, registry
use ractor::{Actor, ActorRef};

struct StrategyCoordinator;
impl Actor for StrategyCoordinator { ... }

// Data plane (market data, order execution)
// Uses custom Arc<T> actors for ultra-low latency
use mycelium_core::{FastActor, FastActorRef};

struct MarketDataProcessor;
impl FastActor for MarketDataProcessor { ... }
```

**Pros**:
- ‚úÖ Get Ractor's supervision for slow path
- ‚úÖ Get custom Arc<T> for fast path
- ‚úÖ Best tool for each job

**Cons**:
- ‚ùå Two actor systems (complexity)
- ‚ùå Interop between them (conversion layer)
- ‚ùå Mental overhead (which system for new actor?)

**Verdict**: Overengineered. Pick one.

---

## Recommendation

### **Build Custom on Tokio**

**Rationale**:

1. **Ractor fork requires 5+ invasive changes**
   - Message trait (+ Sync breaking change)
   - ActorId (3 variants instead of 2)
   - BoxedMessage (3 variants)
   - Transport abstraction (major refactor)
   - Arc downcasting (non-trivial)

2. **Arc<T> is retrofit, not native**
   - Ractor designed for `Box<dyn Any>` and serialization
   - Our changes would fight the architecture
   - Performance won't be optimal

3. **Maintenance burden**
   - Fork diverges from upstream
   - Need to sync security fixes, features
   - Or maintain our own (big commitment)

4. **Custom is cleaner**
   - Design Arc<T> from day 1
   - Simpler codebase (only what we need)
   - Full control over hot path

5. **Effort difference is small**
   - Fork: 6-8 weeks
   - Custom: 4-5 weeks MVP, 8-10 full
   - Custom is actually **faster** to MVP

6. **Ractor's value is supervision**
   - We can implement Erlang-style supervision in ~500 LOC
   - Not worth the fork complexity

---

## If You Still Want to Fork...

### Minimal Viable Fork

**Goal**: Add Arc<T> support with minimal changes

**Approach**:
1. **Add `ArcMessage` wrapper**
   ```rust
   struct ArcMessage(Arc<dyn MessageTrait + Send + Sync>);
   
   trait MessageTrait {
       fn as_any(&self) -> &dyn Any;
   }
   ```

2. **Detect local actors via config**
   ```toml
   [node.local_nodes]
   ids = [1, 2, 3]  # Nodes on same machine
   ```

3. **Modify `box_message()` to check config**
   ```rust
   fn box_message(self, pid: &ActorId) -> BoxedMessage {
       if pid.is_local() {
           // Same process - Box<dyn Any>
       } else if is_local_machine_node(pid.node_id()) {
           // Same machine - Arc<dyn Any>
       } else {
           // Remote - serialize
       }
   }
   ```

**Estimated effort**: 2-3 weeks

**Pros**: Least invasive
**Cons**: Still requires Sync, still requires config management, still drifts from upstream

---

## Conclusion

**The architectural impedance mismatch is real.**

Ractor is designed for:
- Location transparency (serialize everything remote)
- Network-first distributed systems
- Erlang-style message passing

Mycelium needs:
- **Adaptive transparency** (Arc ‚Üí Unix ‚Üí TCP)
- Same-machine optimization (zero-copy)
- HFT latency requirements

**These are fundamentally different design goals.**

**Building custom is the right choice** because:
1. Faster to MVP (4 weeks vs 6-8)
2. Cleaner architecture (designed for Arc<T>)
3. Lower maintenance (no upstream sync)
4. Better performance (native, not retrofit)
5. Full control (no fighting framework)

The only thing Ractor provides that's hard to replicate is **battle-tested supervision trees**. But:
- We can implement Erlang patterns in ~500 LOC
- We control the testing (can be very thorough)
- HFT industry standard is custom anyway

**Ship custom Mycelium runtime on Tokio.** üöÄ
